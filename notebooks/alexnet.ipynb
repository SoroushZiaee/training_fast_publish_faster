{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b429a84-6c7a-4ca3-8fa0-e7599cc7d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a08358-c1d2-4af5-a4e4-fa8d7ad6127f",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b210d15-c03c-4b6d-be19-ddebdd16e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as ch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import (\n",
    "    ToTensor,\n",
    "    ToDevice,\n",
    "    Squeeze,\n",
    "    NormalizeImage,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTorchImage,\n",
    ")\n",
    "from ffcv.fields.rgb_image import (\n",
    "    CenterCropRGBImageDecoder,\n",
    "    RandomResizedCropRGBImageDecoder,\n",
    ")\n",
    "from ffcv.fields.basics import IntDecoder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6bcc981-97f0-40ec-bd9a-4130a23d1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resolution(epoch):\n",
    "    min_res = 160\n",
    "    max_res = 192\n",
    "    end_ramp = 76\n",
    "    start_ramp = 65\n",
    "\n",
    "    assert min_res <= max_res\n",
    "\n",
    "    if epoch <= start_ramp:\n",
    "        return min_res\n",
    "\n",
    "    if epoch >= end_ramp:\n",
    "        return max_res\n",
    "\n",
    "    interp = np.interp([epoch], [start_ramp, end_ramp], [min_res, max_res])\n",
    "    final_res = int(np.round(interp[0] / 32)) * 32\n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a931824-e0a7-4f35-a4cc-c73835d9c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406]) * 255\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225]) * 255\n",
    "DEFAULT_CROP_RATIO = 224 / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03731375-f296-4c9c-b917-f09c1b681edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2502/2502 [05:53<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = \"/home/soroush1/projects/def-kohitij/soroush1/training_fast_publish_faster/data/imagenet_train_256.ffcv\"\n",
    "num_workers = 10\n",
    "batch_size = 512\n",
    "distributed = 0\n",
    "in_memory = 1\n",
    "\n",
    "device = \"cuda:0\" if ch.cuda.is_available() else \"cpu\"\n",
    "train_path = Path(train_dataset)\n",
    "assert train_path.is_file()\n",
    "\n",
    "res = get_resolution(epoch=0)\n",
    "decoder = RandomResizedCropRGBImageDecoder((res, res))\n",
    "image_pipeline = [\n",
    "    decoder,\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    ToDevice(ch.device(device), non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    NormalizeImage(IMAGENET_MEAN, IMAGENET_STD, np.float16),\n",
    "]\n",
    "\n",
    "label_pipeline = [\n",
    "    IntDecoder(),\n",
    "    ToTensor(),\n",
    "    Squeeze(),\n",
    "    ToDevice(ch.device(device), non_blocking=True),\n",
    "]\n",
    "\n",
    "order = OrderOption.RANDOM if distributed else OrderOption.QUASI_RANDOM\n",
    "\n",
    "tr_loader = Loader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    order=order,\n",
    "    os_cache=in_memory,\n",
    "    drop_last=True,\n",
    "    pipelines={\"image\": image_pipeline, \"label\": label_pipeline},\n",
    "    distributed=distributed,\n",
    ")\n",
    "\n",
    "total_tr_data = len(tr_loader)\n",
    "for i, (imgs, lbls) in tqdm(enumerate(tr_loader), total=total_tr_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4d30fcd-6b62-4acc-a42d-799b73d1f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:14<00:00,  6.89it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = \"/home/soroush1/projects/def-kohitij/soroush1/training_fast_publish_faster/data/imagenet_validation_256.ffcv\"\n",
    "num_workers = 10\n",
    "batch_size = 512\n",
    "distributed = 0\n",
    "in_memory = 1\n",
    "resolution = 256\n",
    "\n",
    "device = \"cuda:0\" if ch.cuda.is_available() else \"cpu\"\n",
    "val_dataset = Path(val_dataset)\n",
    "assert val_dataset.is_file()\n",
    "\n",
    "res_tuple = (resolution, resolution)\n",
    "cropper = CenterCropRGBImageDecoder(res_tuple, ratio=DEFAULT_CROP_RATIO)\n",
    "\n",
    "image_pipeline = [\n",
    "    cropper,\n",
    "    ToTensor(),\n",
    "    ToDevice(ch.device(device), non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    NormalizeImage(IMAGENET_MEAN, IMAGENET_STD, np.float16),\n",
    "]\n",
    "\n",
    "label_pipeline = [\n",
    "    IntDecoder(),\n",
    "    ToTensor(),\n",
    "    Squeeze(),\n",
    "    ToDevice(ch.device(device), non_blocking=True),\n",
    "]\n",
    "\n",
    "val_loader = Loader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    order=OrderOption.SEQUENTIAL,\n",
    "    drop_last=False,\n",
    "    pipelines={\"image\": image_pipeline, \"label\": label_pipeline},\n",
    "    distributed=distributed,\n",
    ")\n",
    "\n",
    "total_val_data = len(val_loader)\n",
    "for i, (imgs, lbls) in tqdm(enumerate(val_loader), total=total_val_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c82b90-7e71-4d1b-8b2e-de73e3666bb3",
   "metadata": {},
   "source": [
    "# Create Model and set optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ff42c7c-4f02-4c05-8ea9-f4474cac9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0783a700-c710-47bc-86d6-9695ee3453d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurPoolConv2d(ch.nn.Module):\n",
    "\n",
    "    # Purpose: This class creates a convolutional layer that first applies a blurring filter to the input before performing the convolution operation.\n",
    "    # Condition: The function apply_blurpool iterates over all layers of the model and replaces convolution layers (ch.nn.Conv2d) with BlurPoolConv2d if they have a stride greater than 1 and at least 16 input channels.\n",
    "    # Preventing Aliasing: Blurring the output of convolution layers (especially those with strides greater than 1) helps to reduce aliasing effects. Aliasing occurs when high-frequency signals are sampled too sparsely, leading to incorrect representations.\n",
    "    # Smooth Transitions: Applying a blur before downsampling ensures that transitions between pixels are smooth, preserving important information in the feature maps.\n",
    "    # Stabilizing Training: Blurring can help stabilize training by reducing high-frequency noise, making the model less sensitive to small changes in the input data.\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        default_filter = ch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) / 16.0\n",
    "        filt = default_filter.repeat(conv.in_channels, 1, 1, 1)\n",
    "        self.conv = conv\n",
    "        self.register_buffer(\"blur_filter\", filt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blurred = F.conv2d(\n",
    "            x,\n",
    "            self.blur_filter,\n",
    "            stride=1,\n",
    "            padding=(1, 1),\n",
    "            groups=self.conv.in_channels,\n",
    "            bias=None,\n",
    "        )\n",
    "        return self.conv.forward(blurred)\n",
    "\n",
    "\n",
    "def apply_blurpool(mod: ch.nn.Module):\n",
    "    for name, child in mod.named_children():\n",
    "        if isinstance(child, ch.nn.Conv2d) and (\n",
    "            np.max(child.stride) > 1 and child.in_channels >= 16\n",
    "        ):\n",
    "            setattr(mod, name, BlurPoolConv2d(child))\n",
    "        else:\n",
    "            apply_blurpool(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e535011-b748-4a08-8805-baaaeabc7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"alexnet\"\n",
    "weights = None\n",
    "use_blurpool = True\n",
    "checkpoint = None\n",
    "device = \"cuda:0\" if ch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "scaler = GradScaler()  # since we're using float16, that's why we need to scale our loss\n",
    "model = getattr(models, arch)(weights=weights)\n",
    "\n",
    "\n",
    "if use_blurpool:\n",
    "    apply_blurpool(model)\n",
    "\n",
    "model = model.to(memory_format=ch.channels_last)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b53cc1d1-9f4e-4e2f-994a-3735313dc7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b94d5-002c-42f4-9974-acd1dd3f5eec",
   "metadata": {},
   "source": [
    "# Optimization, Losses, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8019b648-a96d-42de-83a0-2e1e15aa2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9a6e3ba-c959-49e9-a14d-20aa469bb653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k ='features.0.weight'\n",
      "k ='features.0.bias'\n",
      "k ='features.3.weight'\n",
      "k ='features.3.bias'\n",
      "k ='features.6.weight'\n",
      "k ='features.6.bias'\n",
      "k ='features.8.weight'\n",
      "k ='features.8.bias'\n",
      "k ='features.10.weight'\n",
      "k ='features.10.bias'\n",
      "k ='classifier.1.weight'\n",
      "k ='classifier.1.bias'\n",
      "k ='classifier.4.weight'\n",
      "k ='classifier.4.bias'\n",
      "k ='classifier.6.weight'\n",
      "k ='classifier.6.bias'\n"
     ]
    }
   ],
   "source": [
    "# Using SGD as optimizer, using weight decay only for layer that has no batch normalization\n",
    "momentum = 0.9  # pytorch documentation\n",
    "weight_decay = 1e-4  # pytorch documentation weight decay\n",
    "label_smoothing = 0.1  # ffcv documentation\n",
    "lr = 0.01\n",
    "lr_warmup_epochs = 0\n",
    "lr_warmup_decay = 0.01\n",
    "lr_step_size = 30\n",
    "lr_gamma = 0.1\n",
    "lr_warmup_method = \"linear\"\n",
    "device = \"cuda:0\" if ch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "all_params = list(model.named_parameters())\n",
    "bn_params = []\n",
    "other_params = []\n",
    "for k, v in all_params:\n",
    "    if \"bn\" in k:\n",
    "        print(f\"{k =}\")\n",
    "        bn_params.append(v)\n",
    "\n",
    "for k, v in all_params:\n",
    "    if not \"bn\" in k:\n",
    "        print(f\"{k =}\")\n",
    "        other_params.append(v)\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": bn_params, \"weight_decay\": 0.0},\n",
    "    {\"params\": other_params, \"weight_decay\": weight_decay},\n",
    "]\n",
    "\n",
    "optimizer = ch.optim.SGD(param_groups, lr=lr, momentum=momentum)\n",
    "loss = ch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "top1 = torchmetrics.Accuracy(task=\"multiclass\", num_classes=1000).to(device)\n",
    "top5 = torchmetrics.Accuracy(\n",
    "    task=\"multiclass\",\n",
    "    num_classes=1000,\n",
    "    top_k=5,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# scheduler\n",
    "main_lr_scheduler = ch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=lr_step_size, gamma=lr_gamma\n",
    ")\n",
    "\n",
    "if lr_warmup_epochs > 0:\n",
    "    if lr_warmup_method == \"linear\":\n",
    "        warmup_lr_scheduler = ch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=lr_warmup_decay, total_iters=lr_warmup_epochs\n",
    "        )\n",
    "\n",
    "    lr_scheduler = ch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[warmup_lr_scheduler, main_lr_scheduler],\n",
    "        milestones=[lr_warmup_epochs],\n",
    "    )\n",
    "else:\n",
    "    lr_scheduler = main_lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76870a9b-0326-40eb-b373-bca2881cfe8b",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "daf2a55d-f721-4c85-b150-f54ca033acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2429ab8-0129-40bf-b917-6ca85509542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, model, tr_loader, optimizer, loss, scaler, log_level):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    iterator = tqdm(tr_loader)\n",
    "    for ix, (images, target) in enumerate(iterator):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast():\n",
    "            output = model(images)\n",
    "            loss_train = loss(output, target)\n",
    "\n",
    "        scaler.scale(loss_train).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if log_level > 0:\n",
    "\n",
    "            losses.append(loss_train.detach().cpu())\n",
    "            group_lrs = []\n",
    "            for _, group in enumerate(optimizer.param_groups):\n",
    "                group_lrs.append(f'{group[\"lr\"]:.3f}')\n",
    "\n",
    "            top1_acc = top1(output, target)\n",
    "            top5_acc = top5(output, target)\n",
    "\n",
    "            names = [\"ep\", \"iter\", \"shape\", \"lrs\"]\n",
    "            values = [epoch, ix, tuple(images.shape), group_lrs]\n",
    "            names += [\"loss\", \"top1\", \"top5\"]\n",
    "            values += [\n",
    "                f\"{loss_train.item():.3f}\",\n",
    "                f\"{top1_acc.item():.3f}\",\n",
    "                f\"{top5_acc.item():.3f}\",\n",
    "            ]\n",
    "\n",
    "            msg = \", \".join(f\"{n}={v}\" for n, v in zip(names, values))\n",
    "            iterator.set_description(msg)\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def val_loop(epoch, model, tr_loader, optimizer, loss, scaler, log_level):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a562ced-d6b4-4eb2-a7c7-3e620d99bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=0, iter=2501, shape=(512, 3, 160, 160), lrs=['0.010', '0.010'], loss=5.960, top1=0.037, top5=0.086: 100%|██████████| 2502/2502 [10:04<00:00,  4.14it/s]\n",
      "ep=1, iter=2501, shape=(512, 3, 160, 160), lrs=['0.010', '0.010'], loss=5.385, top1=0.072, top5=0.184: 100%|██████████| 2502/2502 [04:33<00:00,  9.15it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "log_level = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_loop(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        tr_loader=tr_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        scaler=scaler,\n",
    "        log_level=log_level,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
