{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.Muri.MuriDataset import MuriDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "imagenet_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "root = \"/home/soroush1/projects/def-kohitij/soroush1/training_fast_publish_faster/data/muri1320\"\n",
    "\n",
    "ds = MuriDataset(root=root, transforms=imagenet_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models.feature_extraction import (\n",
    "    create_feature_extractor,\n",
    "    get_graph_node_names,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class BlurPoolConv2d(torch.nn.Module):\n",
    "\n",
    "    # Purpose: This class creates a convolutional layer that first applies a blurring filter to the input before performing the convolution operation.\n",
    "    # Condition: The function apply_blurpool iterates over all layers of the model and replaces convolution layers (ch.nn.Conv2d) with BlurPoolConv2d if they have a stride greater than 1 and at least 16 input channels.\n",
    "    # Preventing Aliasing: Blurring the output of convolution layers (especially those with strides greater than 1) helps to reduce aliasing effects. Aliasing occurs when high-frequency signals are sampled too sparsely, leading to incorrect representations.\n",
    "    # Smooth Transitions: Applying a blur before downsampling ensures that transitions between pixels are smooth, preserving important information in the feature maps.\n",
    "    # Stabilizing Training: Blurring can help stabilize training by reducing high-frequency noise, making the model less sensitive to small changes in the input data.\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        default_filter = torch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) / 16.0\n",
    "        filt = default_filter.repeat(conv.in_channels, 1, 1, 1)\n",
    "        self.conv = conv\n",
    "        self.register_buffer(\"blur_filter\", filt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blurred = F.conv2d(\n",
    "            x,\n",
    "            self.blur_filter,\n",
    "            stride=1,\n",
    "            padding=(1, 1),\n",
    "            groups=self.conv.in_channels,\n",
    "            bias=None,\n",
    "        )\n",
    "        return self.conv.forward(blurred)\n",
    "    \n",
    "    \n",
    "def apply_blurpool(mod: torch.nn.Module):\n",
    "    for name, child in mod.named_children():\n",
    "        if isinstance(child, torch.nn.Conv2d) and (\n",
    "            np.max(child.stride) > 1 and child.in_channels >= 16\n",
    "        ):\n",
    "            setattr(mod, name, BlurPoolConv2d(child))\n",
    "        else:\n",
    "            apply_blurpool(child)\n",
    "\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    \"\"\"\n",
    "    Remove a prefix from the state_dict keys.\n",
    "\n",
    "    Args:\n",
    "    state_dict (dict): State dictionary from which the prefix will be removed.\n",
    "    prefix (str): Prefix to be removed.\n",
    "\n",
    "    Returns:\n",
    "    dict: State dictionary with prefix removed from keys.\n",
    "    \"\"\"\n",
    "    return {key[len(prefix):]: value for key, value in state_dict.items() if key.startswith(prefix)}\n",
    "\n",
    "def match_and_load_weights(checkpoint_state_dict, model, prefix='module.'):\n",
    "    \"\"\"\n",
    "    Match weights from checkpoint_state_dict with model's state_dict and load them into the model.\n",
    "\n",
    "    Args:\n",
    "    checkpoint_state_dict (dict): State dictionary from checkpoint.\n",
    "    model (torch.nn.Module): The model instance.\n",
    "    prefix (str): Prefix to be removed from checkpoint keys.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Remove the prefix from checkpoint state dict keys\n",
    "    cleaned_checkpoint_state_dict = remove_prefix(checkpoint_state_dict, prefix)\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    matched_weights = {}\n",
    "\n",
    "    # Iterate over the cleaned checkpoint state dict\n",
    "    for ckpt_key, ckpt_weight in cleaned_checkpoint_state_dict.items():\n",
    "        if ckpt_key in model_state_dict:\n",
    "            # If the layer name matches, add to the matched_weights dict\n",
    "            matched_weights[ckpt_key] = ckpt_weight\n",
    "        else:\n",
    "            print(f\"Layer {ckpt_key} from checkpoint not found in the model state dict.\")\n",
    "    \n",
    "    return matched_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1320/1320 [00:07<00:00, 176.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape = (1320, 65536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"resnet50\"\n",
    "checkpoint_path = \"/home/soroush1/projects/def-kohitij/soroush1/training_fast_publish_faster/weights/resnet50-1/checkpoint_epoch_90_0.77.pth\"\n",
    "layer_name = \"layer3.2.bn1\"\n",
    "use_blurpool = True\n",
    "\n",
    "model = getattr(models, model_name)(weights=None)\n",
    "\n",
    "if use_blurpool:\n",
    "    apply_blurpool(model)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "    \n",
    "checkpoint_state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "checkpoint_state_dict = checkpoint_state_dict[\"model_state_dict\"]\n",
    "\n",
    "update_state_dict = match_and_load_weights(checkpoint_state_dict, model)\n",
    "model.load_state_dict(update_state_dict)\n",
    "\n",
    "model = create_feature_extractor(model, return_nodes={layer_name: \"it\"})\n",
    "model.eval()\n",
    "\n",
    "outputs = []\n",
    "for i in tqdm(range(len(ds))):\n",
    "    x = ds[i]\n",
    "    x = x.unsqueeze_(0)\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    if device == \"cpu\":\n",
    "        output = output['it'].flatten(start_dim=0).detach().numpy().reshape(1, -1)\n",
    "    else:\n",
    "        output = output['it'].flatten(start_dim=0).detach().cpu().numpy().reshape(1, -1)\n",
    "    outputs.append(output)\n",
    "\n",
    "outputs = np.concatenate(outputs, axis=0)\n",
    "print(f\"{outputs.shape = }\")\n",
    "# x = torch.cat(imgs, dim=0)\n",
    "# print(f\"{x.size() = }\")\n",
    "\n",
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# output = model(x)\n",
    "\n",
    "# if device == \"cpu\":\n",
    "#     output = output['it'].flatten(start_dim=0).detach().numpy().reshape(1, -1)\n",
    "# else:\n",
    "#     output = output['it'].flatten(start_dim=0).detach().cpu().numpy().reshape(1, -1)\n",
    "# # print time taken to extract features\n",
    "# print(f\"Time taken to extract features: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# print(f\"{output.shape = }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
